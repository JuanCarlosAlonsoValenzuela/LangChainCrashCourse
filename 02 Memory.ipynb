{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a8ecc76-08ff-4aad-987e-3c38d6100064",
   "metadata": {},
   "source": [
    "# Memory\n",
    "**TODO**: Update without deprecation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fdd3aad-cad4-47fd-b637-73d83867b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing OpenAI API key\n",
    "# import os\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976cf38-e030-47fa-bf4c-6ed193aa497d",
   "metadata": {},
   "source": [
    "## 1.- ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229dc6b5-b7ac-49a6-87ae-2a3d7d718df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d307577b-67c6-4297-a972-643331de5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, api_key=os.environ[\"OPENAI_KEY\"])\n",
    "\n",
    "memory = ConversationBufferMemory() # Deprecated\n",
    "\n",
    "conversation = ConversationChain( # Deprecated\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e71979ba-fcc0-45df-828f-59442779b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Juan Carlos\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Juan Carlos! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Juan Carlos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7c8670c-941b-461c-8610-5934acd99400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Juan Carlos\n",
      "AI: Hello Juan Carlos! It's nice to meet you. How can I assist you today?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Juan Carlos.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a5d1c18-3ce9-4992-b069-129178fdd307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Juan Carlos\n",
      "AI: Hello Juan Carlos! It's nice to meet you. How can I assist you today?\n",
      "Human: What is my name?\n",
      "AI: Your name is Juan Carlos.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64107e57-5777-4780-b761-55b0ca0f49dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Juan Carlos\\nAI: Hello Juan Carlos! It's nice to meet you. How can I assist you today?\\nHuman: What is my name?\\nAI: Your name is Juan Carlos.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de66b97-4d2b-4e54-a1ab-c6ed4cf1ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new memory and adding sentences to the conversation\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1830c27-7810-49bb-a56c-5c752b174300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "205437d5-8d95-4b56-bf90-c1570fa21bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up?\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f7356d7-4158-4279-b542-dcf9f145032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new parts of the conversation to the memory\n",
    "memory.save_context({\"input\":\"Not much, just hanging\"},{\"output\":\"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f60af537-1bbc-4a0e-a088-74180b9f0f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up?\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3185c7c3-afc7-4a2f-8ba5-5c50ed39a48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up?\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0af7b7-a62d-4b6e-a908-c7f89bd51d65",
   "metadata": {},
   "source": [
    "## 2.- ConversationBufferWindowMemory\n",
    "It only keeps a window of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8785314-9190-43e2-a28f-b0ee35b23922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c3dba60-82d5-4d22-a55c-0356642bd567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcav\\AppData\\Local\\Temp\\ipykernel_39644\\3135384584.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=1)\n"
     ]
    }
   ],
   "source": [
    "# k is the number of previous conversational exchanges that are remembered by the chatbot\n",
    "memory = ConversationBufferWindowMemory(k=1) # Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36208246-0375-4138-abb5-a6e16370d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up?\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8371451a-19b7-4e6b-8538-a45d837874c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Not much, just hanging\n",
      "AI: Cool\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a18788b6-1fbc-481b-bcfa-fc71fe246633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start a new conversation, this time using ConversationBufferWindowMemory\n",
    "llm = ChatOpenAI(api_key=os.environ[\"OPENAI_KEY\"], temperature=0.0)\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfd0ee06-d462-46a9-9c9a-d21589f250f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Juan Carlos! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Juan Carlos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a37d895-53ec-4499-869e-f8830505227c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Italy is Rome. It is a beautiful city with a rich history dating back thousands of years. Have you ever been there before?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is the capital of Italy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff62a838-691f-4fa6-9ea0-95d675c90b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is the capital of Italy?\n",
      "AI: The capital of Italy is Rome. It is a beautiful city with a rich history dating back thousands of years. Have you ever been there before?\n"
     ]
    }
   ],
   "source": [
    "# My name is no longer present in the memory\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca0336cc-16fb-4789-a9ab-a705d5cc1132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have access to personal information such as your name.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What's my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce179ebc-39c7-42a7-80e8-8d17c0155379",
   "metadata": {},
   "source": [
    "## 3.- ConversationTokenBufferMemory\n",
    "Set a maximum number of tokens to remember."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21c59bb0-905f-415a-b707-8dd67f36e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b18b5244-2843-40ee-8999-3f014f79fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, api_key=os.environ[\"OPENAI_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72f8ceee-06ca-4319-b0fc-d63be1b1debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65bea493-2ba4-46eb-ba24-78f3d887bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c42370cc-4cc5-46a4-bb5e-a065df9e6bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Amazing!\n",
      "Human: Backpropagation is what?\n",
      "AI: Beautiful!\n",
      "Human: Chatbots are what?\n",
      "AI: Charming!\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e460c222-8aed-410d-bb18-95f6ef236081",
   "metadata": {},
   "source": [
    "# 4.- ConversationSummaryBufferMemory\n",
    "The LLM writes a summary of the conversation so far and uses it as its memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3565f517-4aab-4fc5-b22b-00e4fb299c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7da43ad-0a7c-4978-80d9-4b4f4164beaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcav\\AppData\\Local\\Temp\\ipykernel_39644\\1387621936.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n"
     ]
    }
   ],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e78c571-33bf-4e72-996e-e1bca4db66e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human and AI exchange greetings and discuss the schedule for the day. The AI informs the human of a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of being prepared for the day's events.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46aca47e-a18a-4a7e-9c2f-6c1d72ce572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "554dfdd6-7115-4ef5-96c3-59795ed39bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings and discuss the schedule for the day. The AI informs the human of a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of being prepared for the day's events.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"For the morning meeting with the product team, a demo showcasing the latest features and updates on the LangChain project would be ideal. For the lunch meeting with the customer interested in AI, a demo highlighting the capabilities of our AI technology and how it can benefit their specific needs would be impressive. It's important to tailor the demos to each audience to make a strong impact.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08e65f01-21fe-4a40-9a63-a4e8788863d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human and AI exchange greetings and discuss the schedule for the day. The AI informs the human of a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of being prepared for the day's events.\\nHuman: What would be a good demo to show?\\nAI: For the morning meeting with the product team, a demo showcasing the latest features and updates on the LangChain project would be ideal. For the lunch meeting with the customer interested in AI, a demo highlighting the capabilities of our AI technology and how it can benefit their specific needs would be impressive. It's important to tailor the demos to each audience to make a strong impact.\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
